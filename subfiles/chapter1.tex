\documentclass[./dissertation.tex]{subfiles}

\begin{document}

    \contentchapter{Introduction}
    The thesis looks to explore how components of the variational autoencoder can improve the performance of deep metric learning tasks. Chapter two and three will give a greater overview of metric learning and variational autoencoders, respectively. Chapter four will revious previous literature which researches similar or adjacent problems in semi-supervised metric learning and variational autoencoders. Chatper five will introduce a methodology to evaluate the claims we make about how components of the variational autoencoder can be used to extend deep metric learning models to train with supervised data. Chapter six will provide the results of our proposed methods with regards to validity of our claims, and chapter seven will discuss what we can learn from these results and how this line of research can be extended further. The remainder of this chapter gives a brief overview of the motivation and goals for this research. 

    \section{Motivation}
    Within the broader field of representation learning, metric learning is an area which looks to define a distance metric which is smaller between similar objects (such as objects of the same class) and larger between dissimilar objects. Oftentimes, a map is learned from inputs into a low-dimensional latent space where euclidean distance exhibits this relationship, encouraged by training said map against a loss (cost) function based on the euclidean distance between sets of similar and dissimilar objects in the latent space. Unfortunately, existing metric learning methods are generally unable to learn from unlabelled data, which is problematic because unlabelled data is often easier to obtain and is potentially informative.

    \section{Objective}
    We take inspiration from variational autoencoders (VAEs), a generative representation learning architecture, for using unlabelled data to create accurate representations. Specifically, we look to evaluate three atomic claims that detail how pieces of the VAE architecture can create a better deep metric learning (DML) model on a semi-supervised dataset. From here, we can ascertain which specific qualities of how VAEs process unlabelled data are most helpful in modifying DML methods to train with semi-supervised datasets. \\
    
    First, we propose that the autoencoder structure of the VAE, and the associated reconstruction loss, helps the clustering of unlabelled points, as the reconstruction loss may help incorporate semantic information from unlabelled sources. Second, we claim that the structure of the VAE latent space, as it is confined by a prior distribution, can be used to induce bias in the latent space of a DML system. For instance, if we know a dataset contains N-many classes, creating a prior distribution that is a learnable mixture of N gaussians may help produce better representations. Third, we claim that performing DML on the latent space of the VAE so that the DML task can be jointly optimized with the VAE to incorporate unlabelled data may help produce better representations. \\
    
    Each of the four claims will be evaluated experimentally. The claims will be evaluated by comparing a standard DML implementation to the same DML implementation
    \begin{itemize}
        \item jointly optimized with an autoencoder
        \item while structuring the latent space around a prior distribution using the VAEâ€™s latent loss
        \item jointly optimized with a VAE
    \end{itemize}

    \section{Contributions}
    In this work, we aim to provide two main contributions. 
    \begin{enumerate}
        \item We aim to evaluate the following atomic claims for how incorporating components of the variational autoencoder to existing DML architectures may be helpful:
        \begin{enumerate}
            \item including the autoencoder architecture and reconstruction loss term may increase the performance for DML methods on semi-supervised tasks.
            \item including a prior distribution and KL divergence loss term may increase the performance for DML methods on semi-supervised tasks.
            \item jointly optimizing the DML model on the VAE latent space directly may increase the performance for DML methods on semi-supervised tasks.
        \end{enumerate}
        \item We aim to present the results of the joint approaches for VAEs and DML for more recent metric losses that have not been jointly optimized with a VAE in previous literature. 
    \end{enumerate}

\end{document}
