\documentclass[./dissertation.tex]{subfiles}

\begin{document}

\contentchapter{Related Literature}
\SQ{I really think this should be either chapter 1 or 2, or distributed throughout the thesis where related work is relevant. Having a broad ``related literature'' chapter this late in the thesis, splitting up your own work, is very confusing.}

The goal of this research is to investigate how components of the variataional autoencoder can help the performance of deep metric learning in semi supervised tasks. We draw on previous literature to find not only prior attempts at this specific research goal but also work in adjacent research questions that proves insightful. In this review of the literature, we discuss previous related work in the areas of Semi-Supervised Metric Learning, Semi-supervised VAEs, and VAEs with Metric Losses. 


\section{Semi-Supervised Metric Learning}
There have been previous approaches to designing metric learning architectures which incorporate unlabelled data into the metric learning training regimen for semi-supervised datasets. One of the original approaches is the MPCK-MEANS algorithm proposed in the paper Integrating Constraints and Metric Learning in Semi-Supervised Clustering (\cite{bilenko2004integrating}), which adds a penalty for placing labelled inputs in the same cluster which are of a different class or in different clusters if they are of the same class. This penalty is proportional to the metric distance between the pair of inputs. The paper Semi-Supervised Metric Learning Using Pairwise Constraints (\cite{baghshah2009semi}) also looks to impose similar constraints while also introducing a loss term to preserve locally linear relationships between labelled and unlabelled data in the input space. Semi-supervised metric learning via topology preserving multiple semi-supervised assumption (\cite{wang2013semi}) also uses a regularizer term to preserve the topology of the input space. Using variational autoencoders, in a sense, draws on this theme: though there is not explicit term to enforce that the topology of the input space is preserved, a topology of the inputs is intended to be learned through a low-dimensional manifold in the latent space. \\

One more recent common general approach to this problem is to use the unlabelled data's proximity to the labelled data to estimate labels for unlabelled data, effectively "transforming" unlabelled data into labelled data. In Semi-Supervised Metric Learning: A Deep Resurrection (\cite{dutta2021semi}), the authors propose a model which uses affinity propagation on a k-Nearest-Neighbors graph to label partitions of unlabelled data based on their closest neighbors in the latent space. The paper Metric learning by Similarity Network for Deep Semi-Supervised Learning (\cite{wu2020metric}) also looks to assign pseudo-labels to unlabelled data, but not through a graph-based approach. Instead, the proposed model looks to approximate "soft" pseudo-labels for unlabelled that is the metric learning similarity measure between the embedding of unlabelled data and the center of each input of each class of the labelled data. \\

\section{Semi-supervised VAEs}
There have been previous attempts at incorporating labelled information into the variational autoencoder framework. As discussed before, the VAE training regimen does not incorporate training labels -- creating a training regimen for the VAE which does learn from labels is not straightforward. An early solution proposed to this problem is the two models M1 and M2 described in the paper Semi-supervised Learning with Deep Generative Models (\cite{kingma2014autoencoding}). The M1 model trains the VAE on data $X$ without the labels $Y$ to produce encodings $Z$ from and then trains a separate model on a supervised task with the data and labels pair $(Z, Y)$. The M1 model does not actually train the underlying VAE differently, so the authors propose an M2 model, which differs from the vanilla VAE in that there are two encoders which produce not only the latent vector $z$ for each datapoint $x$ but also a predicted label $y'$, both of which the decoder receives as input. The classification task (i.e. the encoder's prediction $y'$) is trained jointly with the regular VAE loss, as is consistent with the authors' new derivation of the VAE ELBO. \\

A more recent approach to the semi-supervised VAEs discourages producing an explicit label embedding within the latent space. In Capturing Label Characteristics in VAEs (\cite{joy2020capturing}), the authors propose a model which encodes several latent vectors $z_{1}, z_{2}, ... z_{n}$ for $n$ labelled characteristics of the image. For instance, if the dataset was over pictures of people, one characteristic may be if the person was smiling, if they were blonde, etc. For each characteristic $c_{i}$, a classifier is trained to predict $y_{i}$ from only the latent vector $z_{i}$. The authors argue that this is a superior training approach than creating explicit label embeddings with an encoder network as binary labels such as "smiling/not smiling" are oftentimes not actually binary (ex. a picture may show the subject slightly smiling or greatly smiling)

\section{VAEs with Metric Loss}

Some approaches to incorporating labelled data into VAEs use a metric loss to govern the latent space more explicitly. In Deep Variational Metric Learning (\cite{lin2018deep}), the authors model the intra-class invariance (i.e. the class-related information of a data point) and intra-class variance (i.e. the distinct features of a data point not unique to it's class) seperately. Like several other models in this section, this paper's proposed model incorporates a metric loss term for the latent vectors representing intra-class invariance and the latent vectors representing both intra-class invariance and intra-class variance. \\

In ``Deep variational metric learning for transfer of expressivity in multispeaker text to Speech'' (\cite{kulkarni2020deep}) the authors incorporate labelled information into the VAE methodology in two ways. First, a modified architecture called the CVAE is used in which the encoder and generator of the VAE is not only conditioned on the input $X$ and latent vector $z$, respectively, but also on the label $Y$. The CVAE was introduced in previous papers (\cite{sohn2015learning}) (\cite{dahmani2019conditional}). Second, the authors add a metric loss, specifically a multi-class N-pair loss (\cite{sohn2016improved}), in the overall loss function of the model. While it is unclear how the CVAE technique would be adapted in a semi-supervised setting, as there is not a label $Y$ associated with each datapoint $X$, we do add a metric loss to the overall model loss function (albeit a different metric loss). \\
    
Most recently, in High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning (\cite{grosnit2021high}), the authors leverage a new training algorithm for combining VAEs and DML for Bayesian Optimization and said algorithm using simple, contrastive, and triplet metric losses. We look to build on this literature by also testing a combined VAE DML architecture on more recent metric losses, albeit using a simpler training regimen. \\

Lastly, though the paper does not discuss VAEs, it is worthwhile to note Autoencoder-based deep metric learning for network intrusion detection \cite{ANDRESINI2021706}, which proposes a novel combined approach to metric learning (specifically triplet loss) and autoencoders. For a dataset with two classes, two autoencoders are trained on only one class, so triplets can be formed using an anchor point sampled from the dataset, the reconstruction of the autoencoder for the positive class, and the autoencoder for the triplet of the negative class. The authors claim that this method, along with other benefits, do not suffer the convergence problems of many triplet loss DML architectures as the triplets are not randomly sampled.
\end{document}
