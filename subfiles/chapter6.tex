\documentclass[./dissertation.tex]{subfiles}

\begin{document}
    
    \contentchapter{Results}
    
    \section{General Experimental Configuration}
    Each set of experiments shares a common hyperparameter search space. Below we describe the fixed values or sets of values that are included in the search space of each experiment. 
    \subsection{Learning Rate}
    Through informal experimentation, we have found that the learning rate of 0.001 causes the models to converge quickly most consistently. The learning rate is thus set to 0.001 in each experiment.
    \subsection{Latent Space Dimensionality}
    Latent space dimensionality refers to the dimensionality of the vector output of the encoder of a DML network or the dimensionality of the posterior distribution of a VAE (in other words the dimensionality of the latent space). We use the set containing 2 and 10 as potential values for latent space dimensionality. When the latent space dimensionality is 2, we see the added benefit of creating plots of the latent representations (though we can accomplish this through using dimensionality reduction methods like tSNE for higher dimensionalities as well).
    \subsection{Alpha}
    Alpha ($\alpha$) is a hyperapameter which refers to the balance between the unsupervised and supervised losses of some of the modified DML models. More details about the role of $\alpha$ in the model implementations are discussed in the methodology section of the model. We iterate through the set containing 0, 0.25, 0.5, 0.75, 1, which represent 25\% increments in the balance between the unsupervised and supervised loss in semi-supervised training. 
    \subsection{Parital Labels Percentage}
    The partial labels percentage hyperparameter refers to the percentage of the dataset that is labelled and thus the size of the partion of the dataset that can be used for labelled training. Of course, each of the datasets we use is fully labelled, so a partially labelled datset can be trivially constructed by ignoring some of the labels. As the sizes of the dataset vary, each percentage can refer to a different number of labelled samples. We iterate through the set containing 0.01, 0.1, 1, 10, 100, for this hyperparameter (with each value referring to the percentage). 
    \subsection{Datasets}
    Two datasets are used for evaluating the models. The first dataset is MNIST (\cite{lecun-mnisthandwrittendigit-2010}), a very popular dataset containing greyscale images of handwritten digits. The second dataset we use is the organ OrganAMNIST dataset from MedMNIST v2 (\cite{medmnistv2}). This dataset contains 2D slices from computed tomography images from the Liver Tumor Segmentation Benchmark -- the labels correspond to the classification of 11 different body organs. The decision to use a second dataset was motivated because the as the claims are tested over more datasets, the claims themselves are more generalizable. The decision to use the OrganAMNIST datasets specifically is motivated in part due to the the Quinn Research Group working on similar tasks for biomedical imaging \cite{Zain2020TowardsAU}. It is also motivated in part because OrganAMNIST is a more difficult dataset, at least for a the classfication task, as the leading accuracy for MNIST is .9991 (\cite{DBLP:journals/corr/abs-2008-10400}) while the leading accuracy for OrganAMNIST is .951 (\cite{medmnistv2}). The MNIST and OrganAMNIST datasets are similar in dimensionality (1 x 28 x 28), number of samples (60,000 and 58,850, respectively) and in that they are both greyscale.
    
    \section{Evaluation}
    We will evaluate the results by running each model on a test partition of data. We then take the latent points $Z$ generated by the model and the corresponding labels $Y$. Three classifiers (sklearn's implementation of RandomForest, MLP, and kNN) each output predicted labels $\hat{Y}$ for the latent points. We finally measure the quality of the predicted labels $\hat{Y}$ using the Adjusted Mutual Information Score \cite{vinh2010information} (which appears to be used in the literature for this context \cite{zhu2021finding} \cite{emmons2016analysis}) and accuracy (which is still helpful but is also more interpretable). The performance of a classifier on the latent points intuitively can be used as a measure of quality of clustering.
    \section{Claim 1: Benefits of Reconstruction Loss}
    In evaluating the first claim, we compare the performance of the plain DML model to the DML Autoencoder model. 
    \section{Claim 2: Incorporating Inductive Bias with Prior}
    In evaluating the second claim, we compare the performance of the plain DML model to the DML with a unit prior and a DML with a GMM prior. We also compare the performance to a DML with a VampPrior. Gaging the performance of a DML with a learnable prior like a VampPrior is not as key to evaluating whether inductive bias can be incorporated because the VampPrior not only incorporates inductive bias but adds complexity, while the unlearned priors only incorporate inductive bias. However, we still run experiments with the DML with VampPrior as we feel it is in line with this thesis' goals to assess how components of VAE and modified VAE models can augment and improve the standard DML model. \\ 
    - Assign each cluster to a psuedoinput using KNN. Then plot the percentages of each class within each pseudoinput for each pseudoinput. Measure the homogenity for each pseudoinput to see if relationship between that and mutual inforamtion score.
    \section{Claim 3: Jointly Optimizing DML with VAE}
    - DML vs VAE vs VAE Auteoncoder \\
    - Different Losses
    
    
\end{document}
