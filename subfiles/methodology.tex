\documentclass[./dissertation.tex]{subfiles}
\usepackage{algorithm2e}
\begin{document}

    \contentchapter{Methodology}
    We look to discover the potential of applying components of the VAE methodology to DML systems. We test this through presenting incremental modifications to the basic DML architecture. Each modified architecture corresponds to a claim about how a specific part of the VAE training regime and loss function may be adapted to assist the performance of a DML method for a semi-supervised dataset.
    
    \RestyleAlgo{ruled}
    \SetKwComment{Comment}{/* }{ */}

    \begin{algorithm}
    \caption{Base DML Training Routine}\label{alg:two}
    \KwIn{Dataset dataset $D$, encoder network $f$, metric loss function $m$, learning rate $\gamma$, weights $\theta$}
    \KwResult{updated weights $\theta$}
    \For{batch $x$, $y$ in $D$}{
        $z = f_{\theta}(x)$\;
        $c = m(z, y)$\;
        Compute $\frac{dc}{d\theta}$ with backpropogation\;
        $\theta = \theta - \gamma \frac{dc}{d\theta}$\;
    }
    \end{algorithm}
     
    \section{Claim 1: Benefits of Reconstruction Loss}
    We first look to evaluate the claim that adding a reconstruction loss to a DML system can improve the quality of clustering in the latent representations on a semi-supervised dataset. Reconstruction loss in and of itself enforces a similar semantic mapping onto the latent space as a metric loss, but can be computed  without labelled data. In theory, we believe that the added constraint that the latent vector must be reconstructed to approximate the original output will train the spatial positioning to reflect semantic information. Following this reasoning, observations which share similar semantic information, specifically observations of the same class (even if not labelled as such), should intuitively be positioned nearby within the latent space. To test if this intuition occurs in practice, we evaluate if a DML model with an autoencoder structure and reconstruction loss (described in further detail below) will perform better than a plain DML model in terms of clustering quality. This will be especially evident for semi-supervised datasets in which the amount of labelled data is not feasible for solely supervised DML. \\
    
    Given a semi-supervised dataset, we assume a standard DML system will use only the labelled data and train given a metric loss $L_{metric}$ (see Algorithm 1). Our modified model DML Autoencoder will extend the DML model's training regime by adding a decoder network which takes the latent point $z$ as input and produces an output $\hat{x}$. The loss function is then modified such that there is a supervised loss $L_{S}$ which is identical to the metric loss $L_{metric}$ and an unsupervised loss that is identical to the reconstruction loss $L_{U}$. Each epoch, the total loss alternates between the supervised and unsupervised loss, such $L = (1 - \alpha) L_{S} + \alpha L_{U}$ on odd number epochs and $L = \alpha L_{U}$ for even number epochs. $\alpha$ is a hyperparameter which modulates the impact of the reconstruction loss on total loss for the DML autoencoder. The software tool used, Pytorch Lightning (\cite{Falcon_PyTorch_Lightning_2019}), used to construct the models restricts not using all parameters in the computation of the loss for a given epoch; thus we have a semi-supervised stage consisting of the unsupervised and the supervised loss instead of solely a supervised stage, as the the supervised loss does not make use of the parameters in the decoder. \\

    \begin{algorithm}
    \caption{DML Autoencoder Training Routine}\label{alg:two}
    \KwIn{Dataset dataset $D$, encoder network $f$, decoder network $g$, metric loss function $m$, learning rate $\gamma$, coefficient $\alpha$, weights $\theta$}
    \KwResult{updated weight $\theta$}
    \For{batch $x$, $y$ in $D$}{
        $z = f_{\theta}(x)$\;
        $\hat{x} = g_{\theta}(z)$\;
        \eIf{s} {
            $c = (1 - \alpha) * m(z, y) + \alpha * MSE(\hat{x}, x)$\;
        }
        {
            $c = \alpha * MSE(\hat{x}, x)$\;
        }
        Compute $\frac{dc}{d\theta}$ with backpropogation\;
        $\theta = \theta - \gamma \frac{dc}{d\theta}$\;
    }
    \end{algorithm}    
    
    \section{Claim 2: Incorporating Inductive Bias with Prior}
    Say we are aware that a dataset has $n$ classes. It may be useful to encourage that there are $n$ clusters in the latent space of a DML model. This can be enforced by using a prior distribution containing $n$ many Gaussians. As we wish to measure only the affect of inducing bias on the representation without adding any complexity to the model, the prior distribution will not be learnable (unlike VAE with VampPrior). By testing whether the classes of points in the latent space are organized along the prior components we can test whether bias can be induced using a prior to constrain the latent space of a DML. By testing whether clustering improves performance, we can evaluate whether this inductive bias is helpful. \\
    
    Given a fully supervised dataset, we assume a standard DML system will use only the labelled data and train given a metric loss $L_{metric}$. Our modified model will extend the DML system's training regime by adding a KL divergence term to the loss which measures the difference between posterior distributions and a prior distribution. It should also be noted that, like the VAE encoder, we will map the input not to a latent point but to a latent distribution. The latent point is stochastically sampled from the latent distribution during training. Mapping the input to a distribution instead of a point will allow us to calculate the KL divergence. \\
    
    The loss function is then modified such that the total loss $L$ is equal to a weighted sum between the metric loss term $L_{metric}$ and the KL divergence term $L_{KL}$. As is true in the previous section, the total loss alternates between the supervised and unsupervised loss, such $L = (1 - \alpha) L_{S} + \alpha L_{U}$ on odd number epochs and $L = \alpha L_{U}$ for even number epochs. \\
    
    In practice, we will be evaluating a DML model with a unit prior and a DML model with a mixture of gaussians (GMM) prior. The latter model constructs the prior as a mixture of $n$ gaussians -- each the vertice of the unit (i.e. each side is 2 units long) hypercube in the latent space. The logvar of each component is set equal to one. Constructing the prior in this way is beneficial in that it is ensured that each component is evenly spaced within the latent space, but is limiting in that there must be exactly $2^{d}$ components in the GMM prior. Thus, to test, we will test a datset with 10 classes on the latent space dimensionality of 4, such that there are $2^{4} = 16$ gaussian components in the GMM prior. Though the number of prior components is greater than the number of classes, the latent mapping may still exhibit the pattern of classes forming clusters around the prior components as the extra components may be made redundant. \\
    
    The drawback of the decision to set the GMM components' means to the coordinates of the unit hypercube's vertices is that the manifold of the chosen dataset may not necessarily exist in 4 dimensions. Choosing gaussian components from a d-dimensional hypersphere in the latent space $\mathcal{R}^{d}$ would solve this issue, but there does not appear to be a solution for choosing $n$ evenly spaced points spanning $d$ dimensions on a $d$-dimensional hypersphere. KL Divergence is calculated with a monte carlo approximation for the GMM and analytically with the unit prior.
    
    \begin{algorithm}
    \caption{DML with Prior Training Routine}\label{alg:two}
    \KwIn{Dataset dataset $D$, encoder network $f$, metric loss function $m$, learning rate $\gamma$, coefficient $\alpha$, prior distribution mean and log-variance $\mu_{p}$, $\sigma_{p}$, weights $\theta$}
    \KwResult{updated weights $\theta$}
    \For{batch $x$, $y$ in $D$}{
        $\mu, \sigma = f_{\theta}(x)$\;
        $z \sim N(\mu, \sigma)$\;
        \eIf{s} {
            $c = (1 - \alpha) * m(z, y) + \alpha * KL(z, \mu, \sigma, \mu_{p}, \sigma_{p})$\;
        }
        {
            $c = \alpha * KL(z, \mu, \sigma, \mu_{p}, \sigma_{p})$\;
        }
        Compute $\frac{dc}{d\theta}$ with backpropogation\;
        $\theta = \theta - \gamma \frac{dc}{d\theta}$\;
    }
    \end{algorithm}
    \begin{algorithm}
    \caption{Monte-Carlo KL Divergence Algorithm}\label{alg:two}
    \KwIn{Latent variable $z$, approximate posterior distribution mean and variance $\mu$,$\sigma$, prior distribution mean and log-variance $\mu_{p}$, $\sigma_{p}$}
    \KwResult{KL Divergence between distributions $q$ and $p$}
    $P(z|\mu, \sigma) = -0.5 * \frac{(z - \mu)^{2}}{\exp{\sigma}}$\;
    $Q(z|\mu_{p}, \sigma_{p}) = -0.5 * \frac{(z - \mu_{p})^{2}}{\exp{\sigma_{p}}}$\;
    \Return $Q(z|\mu_{p}, \sigma_{p}) - P(z|\mu, \sigma)$
    \end{algorithm}
    
    \section{Claim 3: Jointly Optimizing DML with VAE}
    The third claim we look to evaluate is that given a semi-supervised dataset, optimizing a DML model jointly with a VAE on the VAEâ€™s latent space will produce superior clustering than the DML model individually. The intuition behind this approach is that DML methods can learn from only supervised data and VAE methods can learn from only unsupervised data; the proposed methodology will optimize both tasks simultaneously to learn from both supervised and unsupervised data.
    
    The MetricVAE implementation we create jointly optimizes the VAE task and DML task on the VAE latent space. Across epochs, the MetricVAE model alternates between training only the unsupervised task $L_{U}$ and the semi-supervised task $\alpha * L_{U} + (1 - \alpha) * L_{S}$, like each of the other modified DML models. The actual implementation belies the pseudocode algorithm slightly as it uses the VAE with VampPrior model instead of the vanilla VAE.  
    
    \begin{algorithm}
    \caption{DML VAE Training Routine}\label{alg:two}
    \KwIn{Dataset dataset $D$, encoder network $f$, decoder network $g$, metric loss function $m$, learning rate $\gamma$, coefficient $\alpha$, coefficient $\beta$, prior distribution mean and log-variance $\mu_{p}$, $\sigma_{p}$, weights $\theta$}
    \KwResult{updated weights $\theta$}
    \For{batch $x$, $y$ in $D$}{
        $\mu, \sigma = f_{\theta}(x)$\;
        $z \sim N(\mu, \sigma)$\;
        $\hat{x} = g_{\theta}(z)$\;
        $c_{vae} = MSE(x, x_{hat}) + \beta * KL(z, \mu, \sigma, \mu_{p}$\;
        \eIf{s} {
            $c = (1 - \alpha) * m(z, y) + \alpha * c_{vae}$\;
        }
        {
            $c = \alpha * c_{vae}$\;
        }
        Compute $\frac{dc}{d\theta}$ with backpropogation\;
        $\theta = \theta - \gamma \frac{dc}{d\theta}$\;
    }
    \end{algorithm}

    \begin{figure}[h]
        \centering\includegraphics[width=0.5\textwidth]{figures/DML_Arcs.drawio.png}
        \caption{Comparison of Modified DML Architectures}
        \label{Triplet Loss Diagram}
    \end{figure}

    
\end{document}
